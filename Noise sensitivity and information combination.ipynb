{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f164a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import random\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "m=900\n",
    "Nnn=800+800#total number of samples\n",
    "heat_up_number=10#number of warmback stages\n",
    "os.chdir('/kaggle/input/info-combine-0225')\n",
    "y_mm_2=np.array(pd.read_csv('position_heatup_train_2frac_iso_0225.csv'))#fracture labels\n",
    "T_inj_2=np.array(pd.read_csv('T_injection_train_2frac_iso_0225.csv')).reshape(800,m+1,22)[:,:,0]#wellbore temperature at the end of injection\n",
    "T_static_2=np.array(pd.read_csv('2fracture_T_heatup_multistage_New_type_geo_initial_for_0225.csv')).reshape(800,m+1)#static stage temperature\n",
    "XX_2= np.array(pd.read_csv('2fracture_T_heatup_multistage_0225.csv'))#wellbore temperature during warmback\n",
    "real_position_2=np.array(pd.read_csv('2fracture_multistage_realposition_0225.csv'))\n",
    "XX_2=XX_2.reshape(800,m+1,heat_up_number)\n",
    "XX_2=XX_2.transpose(0,2,1)\n",
    "real_position_2=real_position_2.reshape(800,m+1,heat_up_number)\n",
    "real_position_2=real_position_2.transpose(0,2,1)\n",
    "os.chdir('/kaggle/input/inj-warmback-static-0216')\n",
    "y_mm_3=np.array(pd.read_csv('position_heatup_train_2frac_iso_0219.csv'))#(number_samples=100,m=901)\n",
    "T_inj_3=np.array(pd.read_csv('T_injection_train_2frac_iso_0219.csv')).reshape(800,m+1,22)[:,:,0]\n",
    "T_static_3=np.array(pd.read_csv('2fracture_T_heatup_multistage_New_type_geo_initial_for_0219.csv')).reshape(800,m+1)\n",
    "XX_3= np.array(pd.read_csv('2fracture_T_heatup_multistage_0219.csv'))\n",
    "real_position_3=np.array(pd.read_csv('2fracture_multistage_realposition_0219.csv'))\n",
    "Nnn2=800\n",
    "XX_3=XX_3.reshape(Nnn2,m+1,heat_up_number)\n",
    "XX_3=XX_3.transpose(0,2,1)\n",
    "real_position_3=real_position_3.reshape(Nnn2,m+1,heat_up_number)\n",
    "real_position_3=real_position_3.transpose(0,2,1)\n",
    "\n",
    "XX=np.zeros((Nnn-2,heat_up_number,m+1))\n",
    "TT_inj=np.zeros((Nnn-2,m+1))\n",
    "TT_static=np.zeros((Nnn-2,m+1))\n",
    "XX=torch.cat((torch.Tensor(XX_3),torch.Tensor(XX_2[0:435,:,:]),torch.Tensor(XX_2[436:777,:,:]),torch.Tensor(XX_2[778:,:,:])),dim=0)\n",
    "XX=XX.numpy()\n",
    "XX=XX.reshape((Nnn-2)*heat_up_number,m+1)\n",
    "\n",
    "TT_inj=torch.cat((torch.Tensor(T_inj_3),torch.Tensor(T_inj_2[:435,:]),torch.Tensor(T_inj_2[436:777,:]),torch.Tensor(T_inj_2[778:,:])),dim=0)\n",
    "TT_inj=TT_inj.numpy()\n",
    "TT_static=torch.cat((torch.Tensor(T_static_3),torch.Tensor(T_static_2[:435,:]),torch.Tensor(T_static_2[436:777,:]),torch.Tensor(T_static_2[778:,:])),dim=0)\n",
    "TT_static=TT_static.numpy()\n",
    "\n",
    "real_position=np.zeros((Nnn-2,heat_up_number,m+1))\n",
    "real_position=torch.cat((torch.Tensor(real_position_3),torch.Tensor(real_position_2[:435,:,:]),torch.Tensor(real_position_2[436:777,:,:]),torch.Tensor(real_position_2[778:,:,:])),dim=0)\n",
    "real_position=real_position.numpy()\n",
    "real_position=real_position.reshape((Nnn-2)*heat_up_number,m+1)\n",
    "y_mm=np.zeros((Nnn-2,m+1))\n",
    "y_mm=torch.cat((torch.Tensor(y_mm_3),torch.Tensor(y_mm_2[:435,:]),torch.Tensor(y_mm_2[436:777,:]),torch.Tensor(y_mm_2[778:,:])),dim=0)                         \n",
    "y_mm=y_mm.numpy()\n",
    "#interpolate\n",
    "XX_new=np.zeros(((Nnn-2)*heat_up_number,m+1))\n",
    "real_position_new=np.zeros(((Nnn-2)*heat_up_number,m+1))\n",
    "TT_inj_new=np.zeros(((Nnn-2),m+1))\n",
    "TT_static_new=np.zeros(((Nnn-2),m+1))\n",
    "for i in range(0,(Nnn-2)*heat_up_number):\n",
    "    real_position_new[i,0:895]=np.arange(0,895)\n",
    "    real_position_new[i,895]=894.5\n",
    "    real_position_new[i,896]=895\n",
    "    real_position_new[i,897]=895.5\n",
    "    real_position_new[i,898]=896\n",
    "    real_position_new[i,899]=896.5\n",
    "    real_position_new[i,900]=897\n",
    "    real_position_new[i,0]=1.05\n",
    "for i in range(0,(Nnn-2)*heat_up_number):\n",
    "    f = interpolate.interp1d(real_position[i,:], XX[i,:])\n",
    "    XX_new[i,:]=f(real_position_new[i,:]) \n",
    "XX_new=XX_new.reshape((Nnn-2),heat_up_number,m+1)\n",
    "#normalize temp, every sample for all heatup stages uses the same max and min\n",
    "for i in range(Nnn-2):\n",
    "    XX_new[i,:,:]=(XX_new[i,:,:]-np.min(XX_new[i,:,:]))/(np.max(XX_new[i,:,:])-np.min(XX_new[i,:,:]))\n",
    "real_position_new=real_position_new.reshape((Nnn-2),heat_up_number,m+1)\n",
    "#for injection data interpolation\n",
    "real_position=real_position.reshape((Nnn-2),heat_up_number,m+1)\n",
    "for j in range(0,Nnn-2):\n",
    "    ff = interpolate.interp1d(real_position[j,0,:], TT_inj[j,:])\n",
    "    fff=interpolate.interp1d(real_position[j,0,:], TT_static[j,:])\n",
    "    TT_inj_new[j,:]=ff(real_position_new[j,0,:]) \n",
    "    TT_static_new[j,:]=fff(real_position_new[j,0,:]) \n",
    "for i in range(Nnn-2):\n",
    "    TT_inj_new[i,:]=(TT_inj_new[i,:]-np.min(TT_inj_new[i,:]))/(np.max(TT_inj_new[i,:])-np.min(TT_inj_new[i,:])) \n",
    "    TT_static_new[i,:]=(TT_static_new[i,:]-np.min(TT_static_new[i,:]))/(np.max(TT_static_new[i,:])-np.min(TT_static_new[i,:]))        \n",
    "XXXX=np.zeros((Nnn-2,heat_up_number+2,len(XX_new[0,0,:])))#add inj replace xx_new\n",
    "XXXX[:,0,:]=TT_static_new#first input feature: single static stage \n",
    "XXXX[:,1,:]=TT_inj_new#second input feature: single injection stage\n",
    "XXXX[:,2:,:]=XX_new# rest input features: warmback stages\n",
    "Index=random.sample(range(0,Nnn-2),Nnn-2-200)\n",
    "X_train=np.zeros((Nnn-2-200,heat_up_number+2,len(XX_new[0,0,:])))\n",
    "y_train=np.zeros((Nnn-2-200,len(XX_new[0,0,:])))\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for j in range(0,Nnn-2-200):\n",
    "    X_train[j]=XXXX[Index[j],:,:]\n",
    "    y_train[j]=y_mm[Index[j],:]\n",
    "aa=list(np.arange(0,Nnn-2))\n",
    "cc=[x for x in aa if x in Index]\n",
    "dd=[y for y in (aa+list(Index))if y not in cc]\n",
    "for h in range(len(dd)):\n",
    "    XXXX[dd[h],:,:]=XXXX[dd[h],:,:]\n",
    "    X_test.append(XXXX[dd[h],:,:])\n",
    "    y_test.append(y_mm[dd[h],:])\n",
    "y_train_tensors = torch.LongTensor(y_train)\n",
    "y_test_tensors = torch.LongTensor(np.array(y_test))\n",
    "#add noise to input\n",
    "noise_std_input = 0.01\n",
    "X_train_noisy = X_train + noise_std_input * np.random.randn(*np.array(X_train).shape)\n",
    "X_test_noisy=X_test + noise_std_input * np.random.randn(*np.array(X_test).shape)\n",
    "X_train_tensors=torch.Tensor(X_train_noisy[:,2:8,:])\n",
    "X_test_tensors=torch.Tensor(X_test_noisy[:,2:8,:])\n",
    "class TemDataset(Dataset):\n",
    "    def __init__(self,X_train_tensors,y_train_tensors):\n",
    "        self.x=X_train_tensors\n",
    "        self.y=y_train_tensors\n",
    "        self.n_samples=self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "dataset=TemDataset(X_train_tensors,y_train_tensors)\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=256,shuffle=True)#444\n",
    "print(\"Training Shape\", X_train_tensors.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", X_test_tensors.shape, y_test_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate figure 9 in the paper(may be different with the figure in paper because of the randomly chosen training set and the random generated noise)\n",
    "plt.plot(X_train_noisy[399,1,:],-real_position_new[5,1,:],\"r-\",label=\"Injection\",linewidth=0.7)\n",
    "plt.plot(X_train_noisy[399,4,10:],-real_position_new[5,1,10:],\"g-.\",label=\"Warmback\",linewidth=0.7)\n",
    "plt.plot(X_train_noisy[399,0,:],-real_position_new[5,1,:],\"b--\",label=\"Static\",linewidth=0.7)\n",
    "plt.xlabel(\"Dimensionless wellbore temperature/[-]\")\n",
    "plt.ylabel(\"Depth/m\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_feature=6#need to change, depends on how many input features used\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length,embed_size=40*2,heads=1,drop_prob=0):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "        self.embed_size=embed_size\n",
    "        self.heads=heads\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,bidirectional=True,dropout=drop_prob) #lstm\n",
    "        self.gru=nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,dropout=drop_prob)\n",
    "        self.multiattention=nn.MultiheadAttention(embed_size,heads,dropout=drop_prob)\n",
    "        self.ln=nn.LayerNorm(normalized_shape=seq_length,eps=1.0e-5)\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device) #internal state\n",
    "        x=self.ln(x.transpose(2,1))#add layer_norm\n",
    "        x=x.transpose(2,1)\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        out = output\n",
    "        return out    \n",
    "class FCN(nn.Module):\n",
    "    def __init__(self,c_in=Num_feature, c_out=1, layers=[128, 256, 128], kss=[7, 5, 3],embed_size1=128,embed_size2=256,heads=1,drop_prob=0):#c_inæ˜¯feacture number\n",
    "        super(FCN, self).__init__()\n",
    "        assert len(layers) == len(kss)\n",
    "        self.convblock1 = nn.Conv1d(c_in, layers[0], kss[0])\n",
    "        self.convblock2 = nn.Conv1d(layers[0], layers[1], kss[1])\n",
    "        self.convblock3 = nn.Conv1d(layers[1], layers[2], kss[2])\n",
    "        self.gap=torch.nn.AdaptiveAvgPool1d((m+1))\n",
    "        self.fc = nn.Linear((layers[2]+2*40), 10)\n",
    "        self.fcc = nn.Linear(10,2)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.prelu=nn.PReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.bn1=nn.BatchNorm1d(layers[0],eps=1e-03,momentum=0.99)\n",
    "        self.bn2=nn.BatchNorm1d(layers[1],eps=1e-03,momentum=0.99)\n",
    "        self.bn3=nn.BatchNorm1d(layers[2],eps=1e-03,momentum=0.99)\n",
    "        self.dropout=nn.Dropout(p=0)\n",
    "        self.lstm1 = LSTM1(num_classes=m+1, input_size=Num_feature, hidden_size=40, num_layers=2,seq_length=m+1).to(device)#input_size=feacture number\n",
    "    def forward(self, x):\n",
    "        x_fcn = self.convblock1(x)\n",
    "        x_fcn=self.bn1(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.convblock2(x_fcn)\n",
    "        x_fcn=self.bn2(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.convblock3(x_fcn)\n",
    "        x_fcn=self.bn3(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.gap(x_fcn)#size(N,C,m+1)\n",
    "        x_fcn=x_fcn.transpose(1,2)\n",
    "        ####LSTM\n",
    "        x_lstm=x.transpose(1,2)\n",
    "        x_lstm=self.lstm1(x_lstm)\n",
    "        ###concat\n",
    "        out_concat = torch.cat((x_lstm,x_fcn),2)\n",
    "        out_concat=self.prelu(out_concat)\n",
    "        out_concat=self.fc(out_concat)\n",
    "        out_concat=self.prelu(out_concat)\n",
    "        out_concat=self.fcc(out_concat)#size(N,m+1,2)\n",
    "        outt=out_concat.transpose(1,2)#size(N,2,m+1)\n",
    "        out_concat=self.prelu(outt)\n",
    "        out_concat=self.softmax(outt)#calculate along dim=1\n",
    "        return out_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "num_epochs = 800 #changable, depends on the loss history during training\n",
    "learning_rate = 0.005#also changable\n",
    "fcn = FCN().to(device) \n",
    "weight=torch.Tensor([1,100]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weight)#weighted crossentropy loss\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=learning_rate) \n",
    "scheduler=ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=50,\n",
    "verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(inputs,labels) in enumerate(dataloader):  \n",
    "        outputs = fcn.forward(inputs.to(device)) #forward pass\n",
    "        loss = criterion(outputs, labels.to(device)) #forCrossEntropyLoss0411\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "    if epoch % 100 == 0:\n",
    "              losses.append(loss.item())\n",
    "              print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27849184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy matrix:\n",
    "train_predict1 = fcn(X_test_tensors.to(device))#forward pass\n",
    "train_predict1=train_predict1.cpu()\n",
    "data_predict = train_predict1.data.numpy() #numpy conversion\n",
    "dataY_plot = y_test_tensors.data.numpy()\n",
    "data_predict1=data_predict[:,1,:]\n",
    "label_predicted=np.argwhere(np.around(data_predict1)==1)\n",
    "label_real1=np.argwhere(dataY_plot==1)\n",
    "sum_real=0\n",
    "sum_predicted=0\n",
    "for i in range (label_real1.shape[0]):\n",
    "    for any in data_predict1[label_real1[i][0],label_real1[i][1]-4:label_real1[i][1]+5]:\n",
    "        if any>=0.5:\n",
    "            sum_real=sum_real+1\n",
    "            break\n",
    "for j in range(label_predicted.shape[0]):\n",
    "    for any in dataY_plot[label_predicted[j][0],label_predicted[j][1]-4:label_predicted[j][1]+5]:\n",
    "        if any==1:\n",
    "            sum_predicted=sum_predicted+1\n",
    "            break    \n",
    "accuracy=2/(1/(sum_real/label_real1.shape[0])+1/(sum_predicted/label_predicted.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure 8 in the paper\n",
    "#each accuracy point needs a training process\n",
    "plt.plot([0,0.01,0.05,0.1,0.5,1,2],[98.60,99.03,93.59,89.08,79.18,75.67,67.82],\"r--o\",label=\"Injection\",markersize=\"8\",markerfacecolor='none')#99.37\n",
    "plt.plot([0,0.01,0.05,0.1,0.5,1,2],[99.74,100,99.78,99.71,96.49,90.64,78.57],\"g-->\",label=\"Warmback\",markersize=\"8\",markerfacecolor='none')\n",
    "plt.plot([0,0.01,0.05,0.1,0.5,1,2],[99.74,100,99.51,97.71,87.25,78.27,72.42],\"b--*\",label=\"Static\",markersize=\"8\",markerfacecolor='none')#,markeredgewidth=\"1\")#77.96\n",
    "plt.legend()\n",
    "plt.xlabel(\"Noise standard deviation/%\")\n",
    "plt.ylabel(\"Accuracy/%\")\n",
    "#zoom in curve in figure 8\n",
    "plt.plot([0,0.01,0.05,],[98.60,99.03,93.59],\"r--o\",label=\"injection\",markersize=\"8\",markerfacecolor='none')#99.37\n",
    "plt.plot([0,0.01,0.05],[99.74,100,99.78],\"g-->\",label=\"warmback\",markersize=\"8\",markerfacecolor='none')\n",
    "plt.plot([0,0.01,0.05],[99.74,100,99.51],\"b--*\",label=\"static\",markersize=\"8\",markerfacecolor='none')#77.96\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ee787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure 11 in the paper\n",
    "#each accuracy point needs a training process\n",
    "fig = plt.figure()\n",
    "plt.plot([1,2,4,6],[90.64,96.58,98.60,99.24],\"r-.o\",label=\"Warmback\",markersize=\"8\")\n",
    "plt.plot([0,1,2,4,6],[75.67,93.32,97.31,98.74,99.37],\"g-.<\",label=\"Injection+Warmback\",markersize=\"8\")\n",
    "plt.plot([1,2,4,6],[96.47,98.13,98.88,99.37],\"b-.*\",label=\"Injection+Warmback+Static\",markersize=\"8\")#all\n",
    "plt.xlabel(\"Number of warmback stages/[-]\")\n",
    "plt.ylabel(\"Accuracy/%\")\n",
    "fig.legend(loc=3, bbox_to_anchor=(0.5,0.4))#, bbox_transform=ax.transAxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
