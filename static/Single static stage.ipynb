{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1442a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import random\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "#single static stage input data \n",
    "m=900\n",
    "Nnn=800+800\n",
    "heat_up_number=1\n",
    "Nnn1=800\n",
    "os.chdir('/kaggle/input/info-combine-0228')\n",
    "XX_1= np.array(pd.read_csv('2fracture_T_heatup_multistage_New_type_geo_initial_for_0228.csv'))\n",
    "y_mm_1=np.array(pd.read_csv('position_heatup_train_2frac_iso_0228.csv'))\n",
    "real_position_1=np.array(pd.read_csv('2fracture_multistage_realposition_New_type_geo_initial_for_0228.csv'))\n",
    "XX_1=XX_1.reshape(Nnn1,m+1,heat_up_number)\n",
    "XX_1=XX_1.transpose(0,2,1)#(number_samples,heatup_stage,m=901)\n",
    "XX_1=XX_1.reshape(Nnn1*heat_up_number,m+1)\n",
    "real_position_1=real_position_1.reshape(Nnn1,m+1,heat_up_number)\n",
    "real_position_1=real_position_1.transpose(0,2,1)#(number_samples,heatup_stage,m=901)\n",
    "real_position_1=real_position_1.reshape(Nnn1*heat_up_number,m+1)\n",
    "Nnn2=800\n",
    "os.chdir(\"/kaggle/input/iso1219\")\n",
    "XX_2= np.array(pd.read_csv('2fracture_T_heatup_multistage_1217.csv'))\n",
    "y_mm_2=np.array(pd.read_csv('position_heatup_train_2frac_iso_1217.csv'))\n",
    "real_position_2=np.array(pd.read_csv('2fracture_multistage_realposition_1217.csv'))\n",
    "XX_2=XX_2.reshape(Nnn2,m+1,10)[:,:,-2:-1]\n",
    "XX_2=XX_2.transpose(0,2,1)#(number_samples,heatup_stage,m=901)\n",
    "XX_2=XX_2.reshape(Nnn2*heat_up_number,m+1)\n",
    "real_position_2=real_position_2.reshape(Nnn2,m+1,10)[:,:,-2:-1]\n",
    "real_position_2=real_position_2.transpose(0,2,1)#(number_samples,heatup_stage,m=901)\n",
    "real_position_2=real_position_2.reshape(Nnn2*heat_up_number,m+1)\n",
    "\n",
    "XX=np.zeros((Nnn-2,heat_up_number,m+1))\n",
    "XX=torch.cat((torch.Tensor(XX_1),torch.Tensor(XX_2)),dim=0)\n",
    "XX=XX.numpy()\n",
    "XX=XX.reshape((Nnn)*heat_up_number,m+1)\n",
    "XX_new=np.zeros((Nnn*heat_up_number,m+1))\n",
    "real_position=np.zeros((Nnn,heat_up_number,m+1))\n",
    "real_position=torch.cat((torch.Tensor(real_position_1),torch.Tensor(real_position_2)),dim=0)\n",
    "real_position=real_position.numpy()\n",
    "real_position=real_position.reshape((Nnn)*heat_up_number,m+1)\n",
    "y_mm=np.zeros((Nnn,m+1))\n",
    "y_mm=torch.cat((torch.Tensor(y_mm_1),torch.Tensor(y_mm_2)),dim=0)                             \n",
    "y_mm=y_mm.numpy()\n",
    "real_position_new=np.zeros((Nnn*heat_up_number,m+1))\n",
    "\n",
    "\n",
    "for i in range(0,Nnn*heat_up_number):\n",
    "    real_position_new[i,0:897]=np.arange(0,897)\n",
    "    real_position_new[i,897]=896.5\n",
    "    real_position_new[i,898]=897\n",
    "    real_position_new[i,899]=897.5\n",
    "    real_position_new[i,900]=898\n",
    "    real_position_new[i,0]=0\n",
    "\n",
    "#interpolate\n",
    "for i in range(0,Nnn*heat_up_number):\n",
    "    f = interpolate.interp1d(real_position[i,:], XX[i,:])\n",
    "    XX_new[i,:]=f(real_position_new[i,:]) \n",
    "\n",
    "XX_new=XX_new.reshape(Nnn,heat_up_number,m+1)\n",
    "\n",
    "#normalize temp\n",
    "for i in range(Nnn):\n",
    "    XX_new[i,:,:]=(XX_new[i,:,:]-np.min(XX_new[i,:,:]))/(np.max(XX_new[i,:,:])-np.min(XX_new[i,:,:]))\n",
    "#random choose injection/validation data set\n",
    "N_validate=200\n",
    "real_position_new=real_position_new.reshape(Nnn,heat_up_number,m+1)\n",
    "Index=random.sample(range(0,Nnn),Nnn-N_validate)\n",
    "X_train=np.zeros((Nnn-N_validate,heat_up_number,len(XX_new[0,0,:])))\n",
    "y_train=np.zeros((Nnn-N_validate,len(XX_new[0,0,:])))\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for j in range(0,Nnn-N_validate):\n",
    "    X_train[j]=XX_new[Index[j],:,:]\n",
    "    y_train[j]=y_mm[Index[j],:]   \n",
    "aa=list(np.arange(0,Nnn))\n",
    "cc=[x for x in aa if x in Index]\n",
    "dd=[y for y in (aa+list(Index))if y not in cc]\n",
    "for h in range(len(dd)):\n",
    "    XX_new[dd[h],:,:]=XX_new[dd[h],:,:]\n",
    "    X_test.append(XX_new[dd[h],:,:])\n",
    "    y_test.append(y_mm[dd[h],:])\n",
    "\n",
    "\n",
    "y_train_tensors = torch.LongTensor(y_train)\n",
    "y_test_tensors = torch.LongTensor(np.array(y_test))\n",
    "\n",
    "#add noise to input\n",
    "noise_std_input = 0\n",
    "X_train_noisy = X_train + noise_std_input * np.random.randn(*np.array(X_train).shape)\n",
    "X_test_noisy=X_test + noise_std_input * np.random.randn(*np.array(X_test).shape)\n",
    "X_train_tensors=torch.Tensor(X_train_noisy)\n",
    "X_test_tensors=torch.Tensor(X_test_noisy)\n",
    "class TemDataset(Dataset):\n",
    "    def __init__(self,X_train_tensors,y_train_tensors):\n",
    "        self.x=X_train_tensors\n",
    "        self.y=y_train_tensors\n",
    "        self.n_samples=self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset=TemDataset(X_train_tensors,y_train_tensors)\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=256,shuffle=True)#444\n",
    "print(\"Training Shape\", X_train_tensors.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", X_test_tensors.shape, y_test_tensors.shape)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm\n",
    "heat_up_number=1# number of input feacture\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length,embed_size=40*2,heads=1,drop_prob=0):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "        self.embed_size=embed_size\n",
    "        self.heads=heads\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,bidirectional=True,dropout=drop_prob) #lstm\n",
    "        self.gru=nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,dropout=drop_prob)\n",
    "        self.multiattention=nn.MultiheadAttention(embed_size,heads,dropout=drop_prob)\n",
    "        self.ln=nn.LayerNorm(normalized_shape=seq_length,eps=1.0e-5)\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device) #internal state\n",
    "        x=self.ln(x.transpose(2,1))#add layer_norm\n",
    "        x=x.transpose(2,1)\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        out = output\n",
    "        return out    \n",
    "class FCN(nn.Module):\n",
    "    def __init__(self,c_in=heat_up_number, c_out=1, layers=[128, 256, 128], kss=[7, 5, 3],embed_size1=128,embed_size2=256,heads=1,drop_prob=0):\n",
    "        super(FCN, self).__init__()\n",
    "        assert len(layers) == len(kss)\n",
    "        self.convblock1 = nn.Conv1d(c_in, layers[0], kss[0])\n",
    "        self.convblock2 = nn.Conv1d(layers[0], layers[1], kss[1])\n",
    "        self.convblock3 = nn.Conv1d(layers[1], layers[2], kss[2])\n",
    "        self.gap=torch.nn.AdaptiveAvgPool1d((m+1))\n",
    "        self.fc = nn.Linear((layers[2]+2*40), 10)\n",
    "        self.fcc = nn.Linear(10,2)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.prelu=nn.PReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.bn1=nn.BatchNorm1d(layers[0],eps=1e-03,momentum=0.99)\n",
    "        self.bn2=nn.BatchNorm1d(layers[1],eps=1e-03,momentum=0.99)\n",
    "        self.bn3=nn.BatchNorm1d(layers[2],eps=1e-03,momentum=0.99)\n",
    "        self.dropout=nn.Dropout(p=0)\n",
    "        self.lstm1 = LSTM1(num_classes=m+1, input_size=heat_up_number, hidden_size=40, num_layers=2,seq_length=m+1).to(device)\n",
    "    def forward(self, x):\n",
    "        x_fcn = self.convblock1(x)\n",
    "        x_fcn=self.bn1(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.convblock2(x_fcn)\n",
    "        x_fcn=self.bn2(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.convblock3(x_fcn)\n",
    "        x_fcn=self.bn3(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.gap(x_fcn)#size(N,C,m+1)\n",
    "        x_fcn=x_fcn.transpose(1,2)\n",
    "        ####LSTM\n",
    "        x_lstm=x.transpose(1,2)\n",
    "        x_lstm=self.lstm1(x_lstm)\n",
    "        ###concat\n",
    "        out_concat = torch.cat((x_lstm,x_fcn),2)\n",
    "        out_concat=self.prelu(out_concat)\n",
    "        out_concat=self.fc(out_concat)\n",
    "        out_concat=self.prelu(out_concat)\n",
    "        out_concat=self.fcc(out_concat)#size(N,m+1,2)\n",
    "        outt=out_concat.transpose(1,2)#size(N,2,m+1)\n",
    "        out_concat=self.prelu(outt)\n",
    "        out_concat=self.softmax(outt)#calculate softmax along dim=1\n",
    "        return out_concat\n",
    "\n",
    "num_epochs = 800 \n",
    "learning_rate = 0.001\n",
    "fcn = FCN().to(device) \n",
    "weight=torch.Tensor([1,70]).to(device)#100\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=learning_rate) \n",
    "scheduler=ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=50,\n",
    " verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(inputs,labels) in enumerate(dataloader):  \n",
    "        outputs = fcn.forward(inputs.to(device)) \n",
    "        loss = criterion(outputs, labels.to(device)) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "    if epoch % 100 == 0:\n",
    "              losses.append(loss.item())\n",
    "              print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy matrix:\n",
    "train_predict1 = model(X_test_tensors.to(device))#forward pass\n",
    "train_predict1=train_predict1.cpu()\n",
    "data_predict = train_predict1.data.numpy() #numpy conversion\n",
    "dataY_plot = y_test_tensors.data.numpy()\n",
    "data_predict1=data_predict[:,1,:]\n",
    "label_predicted=np.argwhere(np.around(data_predict1)==1)\n",
    "label_real1=np.argwhere(dataY_plot==1)\n",
    "sum_real=0\n",
    "sum_predicted=0\n",
    "for i in range (label_real1.shape[0]):\n",
    "    for any in data_predict1[label_real1[i][0],label_real1[i][1]-4:label_real1[i][1]+5]:\n",
    "        if any>=0.5:\n",
    "            sum_real=sum_real+1\n",
    "            break\n",
    "for j in range(label_predicted.shape[0]):\n",
    "    for any in dataY_plot[label_predicted[j][0],label_predicted[j][1]-4:label_predicted[j][1]+5]:\n",
    "        if any==1:\n",
    "            sum_predicted=sum_predicted+1\n",
    "            break\n",
    "accuracy=2/(1/(sum_real/label_real1.shape[0])+1/(sum_predicted/label_predicted.shape[0]))               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#real case test\n",
    "#load well-trained model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import random\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "m=900\n",
    "Nnn=800#\n",
    "heat_up_number=1\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length,embed_size=40*2,heads=1,drop_prob=0):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "        self.embed_size=embed_size\n",
    "        self.heads=heads\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,bidirectional=True,dropout=drop_prob) #lstm\n",
    "        self.gru=nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,dropout=drop_prob)\n",
    "        self.multiattention=nn.MultiheadAttention(embed_size,heads,dropout=drop_prob)\n",
    "        self.ln=nn.LayerNorm(normalized_shape=seq_length,eps=1.0e-5)\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device) #internal state\n",
    "        x=self.ln(x.transpose(2,1))#add layer_norm\n",
    "        x=x.transpose(2,1)\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        out = output\n",
    "        return out    \n",
    "class FCN(nn.Module):\n",
    "    def __init__(self,c_in=heat_up_number, c_out=1, layers=[128, 256, 128], kss=[7, 5, 3],embed_size1=128,embed_size2=256,heads=1,drop_prob=0):#c_inæ˜¯feacture number\n",
    "        super(FCN, self).__init__()\n",
    "        assert len(layers) == len(kss)\n",
    "        self.convblock1 = nn.Conv1d(c_in, layers[0], kss[0])\n",
    "        self.convblock2 = nn.Conv1d(layers[0], layers[1], kss[1])\n",
    "        self.convblock3 = nn.Conv1d(layers[1], layers[2], kss[2])\n",
    "        self.gap=torch.nn.AdaptiveAvgPool1d((m+1))\n",
    "        self.fc = nn.Linear((layers[2]+2*40), 10)\n",
    "        self.fcc = nn.Linear(10,2)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.prelu=nn.PReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.bn1=nn.BatchNorm1d(layers[0],eps=1e-03,momentum=0.99)\n",
    "        self.bn2=nn.BatchNorm1d(layers[1],eps=1e-03,momentum=0.99)\n",
    "        self.bn3=nn.BatchNorm1d(layers[2],eps=1e-03,momentum=0.99)\n",
    "        self.dropout=nn.Dropout(p=0)\n",
    "        self.lstm1 = LSTM1(num_classes=m+1, input_size=heat_up_number, hidden_size=40, num_layers=2,seq_length=m+1).to(device)#input_size=feacture number\n",
    "    def forward(self, x):\n",
    "        x_fcn = self.convblock1(x)\n",
    "        x_fcn=self.bn1(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.convblock2(x_fcn)\n",
    "        x_fcn=self.bn2(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.convblock3(x_fcn)\n",
    "        x_fcn=self.bn3(x_fcn)\n",
    "        x_fcn=self.relu(x_fcn)\n",
    "        x_fcn = self.gap(x_fcn)#size(N,C,m+1)\n",
    "        x_fcn=x_fcn.transpose(1,2)\n",
    "        ####LSTM\n",
    "        x_lstm=x.transpose(1,2)\n",
    "        x_lstm=self.lstm1(x_lstm)\n",
    "        ###concat\n",
    "        out_concat = torch.cat((x_lstm,x_fcn),2)\n",
    "        out_concat=self.prelu(out_concat)\n",
    "        out_concat=self.fc(out_concat)\n",
    "        out_concat=self.prelu(out_concat)\n",
    "        out_concat=self.fcc(out_concat)#size(N,m+1,2)\n",
    "        outt=out_concat.transpose(1,2)#size(N,2,m+1)\n",
    "        out_concat=self.prelu(outt)\n",
    "        out_concat=self.softmax(outt)#calculate along dim=1\n",
    "        return out_concat\n",
    "    \n",
    "model=torch.load('0315_new_single_new_type_fracture_prediction_99_67.pt')\n",
    "os.chdir('/kaggle/input/ikeda11')\n",
    "m=900\n",
    "realposition=np.array(pd.read_csv('ikeda11.csv'))[0,:]\n",
    "temp=np.array(pd.read_csv('ikeda11.csv'))[1:,:]\n",
    "XX_new1=np.zeros((1,m+1))\n",
    "real_position_new1=np.zeros((1,m+1))\n",
    "real_position_new1[0,0:901]=np.arange(902.108,1585.24,683/900)[0:901]\n",
    "f = interpolate.interp1d(realposition, temp[0,:])\n",
    "XX_new1=(XX_new1-np.min(XX_new1))/(np.max(XX_new1)-np.min(XX_new1))\n",
    "XX_new1=XX_new1.reshape((1,1,m+1))\n",
    "X_test_tensors1= Variable(torch.Tensor(XX_new1))\n",
    "\n",
    "#figure 7 in paper\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "train_predict1 = model(X_test_tensors1.to(device))#forward pass\n",
    "train_predict1=train_predict1.cpu()\n",
    "data_predict = train_predict1.data.numpy() #numpy conversion\n",
    "ax.plot(np.ones(11)*1115,np.arange(0,1.1,0.1),\":\",linewidth=2,color=\"#808080\")    \n",
    "ax.plot(np.ones(11)*1495,np.arange(0,1.1,0.1),\":\",linewidth=2,color=\"#808080\") \n",
    "ax.plot(np.ones(11)*1540,np.arange(0,1.1,0.1),\":\",linewidth=2,color=\"#808080\",label=\"Labeled fractures\") \n",
    "ax.plot(real_position_new1[0,:],np.round(data_predict[0,1,:]),\"r<\",label=\"Predicted fractures\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(real_position_new1[0,:],XX_new1[0,0,:],\"b-\",linewidth=2,label=\"Static temperature\")\n",
    "fig.legend(loc=6, bbox_to_anchor=(0.4,0.25))#, bbox_transform=ax.transAxes)\n",
    "ax.set_ylabel(\"Fracture probability/[-]\",color=\"r\")\n",
    "ax2.set_ylabel(\"Dimensionless wellbore temperature/[-]\",color=\"b\")\n",
    "ax.set_xlabel(\"Depth/m\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
